{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "542ea188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\sumit\\anaconda3\\lib\\site-packages (4.8.0.74)\n",
      "Requirement already satisfied: scikit-image in c:\\users\\sumit\\anaconda3\\lib\\site-packages (0.20.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sumit\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\sumit\\anaconda3\\lib\\site-packages (3.1)\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\users\\sumit\\anaconda3\\lib\\site-packages (from opencv-python) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sumit\\anaconda3\\lib\\site-packages (from scikit-image) (23.1)\n",
      "Requirement already satisfied: imageio>=2.4.1 in c:\\users\\sumit\\anaconda3\\lib\\site-packages (from scikit-image) (2.31.4)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\users\\sumit\\anaconda3\\lib\\site-packages (from scikit-image) (2023.4.12)\n",
      "Requirement already satisfied: pillow>=9.0.1 in c:\\users\\sumit\\anaconda3\\lib\\site-packages (from scikit-image) (10.0.1)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\sumit\\anaconda3\\lib\\site-packages (from scikit-image) (1.4.1)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in c:\\users\\sumit\\anaconda3\\lib\\site-packages (from scikit-image) (0.3)\n",
      "Requirement already satisfied: scipy>=1.8 in c:\\users\\sumit\\anaconda3\\lib\\site-packages (from scikit-image) (1.11.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\sumit\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\sumit\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python scikit-image scikit-learn networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53a7f42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import networkx as nx\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "from scipy.spatial.distance import cdist\n",
    "import time\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edc4eaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "# !pip install opencv-python scikit-image scikit-learn networkx\n",
    "\n",
    "def calculate_ssim(frame1, frame2):\n",
    "    # Convert frames to grayscale\n",
    "    gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "    gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calculate SSIM between the two frames\n",
    "    score, _ = ssim(gray1, gray2, full=True)\n",
    "    return score\n",
    "\n",
    "def extract_frames(video_path):\n",
    "    # Create a directory to store extracted frames\n",
    "    frames_dir = \"extracted_frames\"\n",
    "    os.makedirs(frames_dir, exist_ok=True)\n",
    "\n",
    "    # Read the video and extract frames\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    for frame_number in range(frame_count):\n",
    "        ret, frame = video.read()\n",
    "        if ret:\n",
    "            frame_path = os.path.join(frames_dir, f\"frame_{frame_number}.jpg\")\n",
    "            cv2.imwrite(frame_path, frame)\n",
    "\n",
    "    video.release()\n",
    "\n",
    "    return frames_dir\n",
    "\n",
    "def calculate_ccfv(color_feature, edge_feature, wavelet_feature):\n",
    "    # Initialization\n",
    "    X = color_feature\n",
    "    Y = edge_feature + wavelet_feature\n",
    "\n",
    "    # Covariance Matrices\n",
    "    Sxx = np.cov(X, rowvar=False)\n",
    "    Syy = np.cov(Y, rowvar=False)\n",
    "    Sxy = np.cov(X, Y, rowvar=False)\n",
    "\n",
    "    # Compute Transformation Matrices G1 and G2\n",
    "    G1 = np.dot(np.dot(np.linalg.inv(np.sqrt(Sxx)), Sxy), np.dot(np.linalg.inv(Syy), Sxy.T))\n",
    "    G2 = np.dot(np.dot(np.linalg.inv(np.sqrt(Syy)), Sxy.T), np.dot(np.linalg.inv(Sxx), Sxy))\n",
    "\n",
    "    # Compute Eigen Vectors and Rank\n",
    "    _, u = np.linalg.eigh(G1)\n",
    "    _, v = np.linalg.eigh(G2)\n",
    "    r = np.linalg.matrix_rank(Sxy)\n",
    "\n",
    "    # Choose Canonical Variables\n",
    "    d = 100  # Set to a suitable value\n",
    "    Wx = np.dot(np.linalg.inv(np.sqrt(Sxx)), u[:, :d])\n",
    "    Wy = np.dot(np.linalg.inv(np.sqrt(Syy)), v[:, :d])\n",
    "\n",
    "    # Compute Canonically Correlated Feature Vector (CCFV)\n",
    "    CCFV = np.dot(Wx.T, X) + np.dot(Wy.T, Y)\n",
    "\n",
    "    return CCFV\n",
    "\n",
    "def graph_modularity_clustering(similarity_graph, edge_set, weight_set, num_frames, scaling_parameter):\n",
    "    # Iterative Edge Pruning\n",
    "    for i in range(num_frames):\n",
    "        for j in range(num_frames):\n",
    "            # Calculate the difference Δij\n",
    "            delta_ij = similarity_graph[i, j] - scaling_parameter\n",
    "            # Prune edges with high Δ values\n",
    "            if delta_ij > 0:\n",
    "                similarity_graph[i, j] = 0\n",
    "\n",
    "    # Edge Pruning and Clustering Enhancement\n",
    "    while True:\n",
    "        # Find connected components within the VSG\n",
    "        n_components, labels = connected_components(similarity_graph, directed=False)\n",
    "        prev_modularity = calculate_modularity(similarity_graph, labels)\n",
    "\n",
    "        # Calculate Dev for each edge\n",
    "        dev = calculate_dev(similarity_graph, edge_set, weight_set, scaling_parameter)\n",
    "        # Select edges with high Dev values and remove them\n",
    "        for i, j in edge_set:\n",
    "            if dev[i, j] > 0:\n",
    "                similarity_graph[i, j] = 0\n",
    "\n",
    "        # Calculate modularity after pruning\n",
    "        n_components, labels = connected_components(similarity_graph, directed=False)\n",
    "        modularity = calculate_modularity(similarity_graph, labels)\n",
    "\n",
    "        # Check for improvement in modularity\n",
    "        if modularity - prev_modularity < 1e-6:\n",
    "            break\n",
    "\n",
    "    # Obtain Individual Clusters\n",
    "    n_components, labels = connected_components(similarity_graph, directed=False)\n",
    "\n",
    "    return labels\n",
    "\n",
    "def calculate_modularity(graph, labels):\n",
    "    num_edges = np.sum(graph) / 2\n",
    "    modularity = 0\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "            if labels[i] == labels[j]:\n",
    "                modularity += (graph[i, j] - np.sum(graph[i, :]) * np.sum(graph[:, j]) / (2 * num_edges))\n",
    "\n",
    "    modularity /= (2 * num_edges)\n",
    "\n",
    "    return modularity\n",
    "\n",
    "def calculate_dev(similarity_graph, edge_set, weight_set, scaling_parameter):\n",
    "    dev = np.zeros_like(similarity_graph)\n",
    "    for i, j in edge_set:\n",
    "        dev[i, j] = similarity_graph[i, j] - scaling_parameter\n",
    "\n",
    "    return dev\n",
    "\n",
    "def summarize_video_multi_feature(video_path, num_frames, batch_size=100):\n",
    "    frames_dir = extract_frames(video_path)\n",
    "\n",
    "    keyframes = []\n",
    "    prev_frame = None\n",
    "\n",
    "    for frame_number in range(num_frames):\n",
    "        frame_path = os.path.join(frames_dir, f\"frame_{frame_number}.jpg\")\n",
    "        current_frame = cv2.imread(frame_path)\n",
    "\n",
    "        if prev_frame is not None:\n",
    "            # Extract features from Color, Edge, and Wavelet\n",
    "            color_feature = prev_frame.flatten()\n",
    "            edge_feature = cv2.Canny(cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY), 50, 150).flatten()\n",
    "            wavelet_feature = cv2.dft(cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY).astype(float), flags=cv2.DFT_COMPLEX_OUTPUT).flatten()\n",
    "\n",
    "            # Calculate Canonically Correlated Feature Vector (CCFV)\n",
    "            ccfv = calculate_ccfv(color_feature, edge_feature, wavelet_feature)\n",
    "\n",
    "            # Append CCFV to keyframes\n",
    "            keyframes.append(ccfv)\n",
    "\n",
    "        prev_frame = current_frame\n",
    "\n",
    "    # Convert keyframes to numpy array\n",
    "    keyframes = np.array(keyframes)\n",
    "\n",
    "    # Perform Graph Modularity Clustering\n",
    "    similarity_graph = cdist(keyframes, keyframes, metric='cosine')\n",
    "    np.fill_diagonal(similarity_graph, 0)  # Set diagonal elements to zero\n",
    "    edge_set = np.column_stack(np.where(similarity_graph > 0))\n",
    "    weight_set = similarity_graph[edge_set[:, 0], edge_set[:, 1]]\n",
    "    labels = graph_modularity_clustering(similarity_graph, edge_set, weight_set, num_frames, scaling_parameter=0.1)\n",
    "\n",
    "    # Create the \"process_frames\" directory and move keyframes there\n",
    "    process_frames_dir = os.path.join(frames_dir, \"process_frames\")\n",
    "    os.makedirs(process_frames_dir, exist_ok=True)\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        frame_path = os.path.join(frames_dir, f\"frame_{i}.jpg\")\n",
    "        target_path = os.path.join(process_frames_dir, f\"frame_{i}_cluster_{label}.jpg\")\n",
    "        try:\n",
    "            os.rename(frame_path, target_path)\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "\n",
    "    selected_frames = [os.path.join(process_frames_dir, f\"frame_{i}_cluster_{label}.jpg\") for i, label in enumerate(labels)]\n",
    "\n",
    "    # Extract keyframes closest to centroids\n",
    "    keyframes = []\n",
    "    for cluster_label in np.unique(labels):\n",
    "        cluster_indices = np.where(labels == cluster_label)[0]\n",
    "        cluster_centroid = np.mean(keyframes[cluster_indices], axis=0)\n",
    "        closest_frame_index = np.argmin(np.linalg.norm(keyframes[cluster_indices] - cluster_centroid, axis=1))\n",
    "        keyframes.append(selected_frames[cluster_indices[closest_frame_index]])\n",
    "\n",
    "    # Arrange keyframes in temporal order\n",
    "    keyframes.sort(key=lambda x: int(x.split('_')[1]))\n",
    "\n",
    "    return keyframes\n",
    "\n",
    "def create_summarized_video(input_video_path, output_video_path, selected_frames):\n",
    "    # Get the video properties from the input video\n",
    "    input_video = cv2.VideoCapture(input_video_path)\n",
    "    frame_width = int(input_video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(input_video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(input_video.get(cv2.CAP_PROP_FPS))\n",
    "    video_codec = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "\n",
    "    # Create the output video writer\n",
    "    output_video = cv2.VideoWriter(output_video_path, video_codec, fps, (frame_width, frame_height))\n",
    "\n",
    "    # Read the selected frames and write them to the output video\n",
    "    for frame_path in selected_frames:\n",
    "        frame = cv2.imread(frame_path)\n",
    "        output_video.write(frame)\n",
    "\n",
    "    # Release resources\n",
    "    input_video.release()\n",
    "    output_video.release()\n",
    "\n",
    "def get_time_complexity():\n",
    "    start_time = time.time()\n",
    "    # Input video path\n",
    "    input_video_path = r'C:\\Users\\Sumit\\Desktop\\Test Results 01\\Surveilance_camera_test01.mp4'\n",
    "\n",
    "    # Specify the desired number of resultant frames\n",
    "    num_frames = 10\n",
    "\n",
    "    try:\n",
    "        # Summarize the video and obtain selected frames\n",
    "        selected_frames = summarize_video_multi_feature(input_video_path, num_frames)\n",
    "\n",
    "        # Output video path\n",
    "        output_video_path = os.path.join(os.path.dirname(input_video_path), 'C:/Users/Sumit/Desktop/Test Results 01/Summarized_Surveilance_camera_test01.mp4')\n",
    "\n",
    "        # Create the summarized output video\n",
    "        create_summarized_video(input_video_path, output_video_path, selected_frames)\n",
    "\n",
    "        print(\"Video summarization completed successfully!\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Video summarization failed: {str(e)}\")\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return elapsed_time\n",
    "\n",
    "def get_space_complexity():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_usage = process.memory_info().rss  # in bytes\n",
    "    memory_usage_mb = memory_usage / (1024 ** 2)  # convert to megabytes\n",
    "    return memory_usage_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c003958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video summarization failed: operands could not be broadcast together with shapes (230400,) (460800,) \n",
      "Overall Time Complexity: 13.8375883102417 seconds\n",
      "Overall Space Complexity: 154.21875 MB\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    time_complexity = get_time_complexity()\n",
    "    space_complexity = get_space_complexity()\n",
    "\n",
    "    print(f\"Overall Time Complexity: {time_complexity} seconds\")\n",
    "    print(f\"Overall Space Complexity: {space_complexity} MB\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
